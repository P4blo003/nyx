# ==========================================================================================
# Author: Pablo González García.
# Created: 31/12/2025
# Last edited: 15/01/2026
#
# Environment configuration file for Triton Inference Server deployments.
# This file defines common settings and service-specific variables for:
# - Triton + vLLM (LLM Inference).
# - Triton + ONNX (classical ML / DL Inference).
# ==========================================================================================


# ==============================
# GRAFANA
# ==============================

# Disable anonymous access to prevent unauthorized users for viewing
# or modifying dashboards. This is critical for production and industrial
# deployments where sensitive metrics and system information are exposed.
GF_AUTH_ANONYMOUS_ENABLED=false
# Enable GZIP compression for Grafana server responses.
# THis reduces bandwidth usage and improves UI responsiveness,
# especially when visualizing large dashboards or many metrics.
GF_SERVER_ENABLE_GZIP=true


# ==============================
# COMMON
# ==============================

# Triton Inference Server version to use.
# This must match the NVIDIA container tag (nvcr.io/nvidia/tritonserver).
# See https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags to se available versions.
TRITON_VERSION=25.02

# Nvidia driver capabilities enabled inside the container.
# - Compute: CUDA compute support.
# - Utility: Nvidia-smi and monitoring.
# - Video: Required for some multimedia / vision pipelines.
NVIDIA_DRIVER_CAPABILITIES=compute,utility,video

# Controls Triton model configuration strictness.
# - False: Triton will auto-complete missing fields in config.pbtxt.
# - True: Triton will refuse models without a complete config.pbtxt.
# In production, true is recommended once configs are stable.
TRITON_STRICT_MODEL_CONFIG=false


# ==============================
# VLLM
# ==============================

# CUDA devices(s) visible to the vLLM Triton container.
# Typically a single, dedicated GPU is assigned to the LLM workload.
VLLM_CUDA_VISIBLE_DEVICES=0
# Attention backend used by vLLM.
# XFORMERS is widely supported and stable.
# FLASHINFER or FLASH_ATTENTION may provide better performance on newer GPUs.
VLLM_ATTENTION_BACKEND=XFORMERS
# Fraction of GPU memory that vLLM is allowed to allocate.
# Leaving headroom avoids OOM errors and allows CUDA context stability.
# Typical production values: 0.80 - 0.90
VLLM_GPU_MEMORY_UTILIZATION=0.85
# Maximum context length (tokens) supported by the LLM.
# This has a direct impact on KV cache size and GPU memory usage.
# Set this as low as your use case allows.
VLLM_MAX_MODEL_LEN=8192


# ==============================
# ONNX
# ==============================

# CUDA device(s) visible to the Triton ONNX container.
# When sharing a GPU with vLLM, ONNX models should be lightweight and well-batched.
# In multi-GPU setups, this is often assigned to a different device.
ONNX_CUDA_VISIBLE_DEVICES=0


# ==============================
# AI SERVICE
# ==============================

# HTTP endpoint port.
AI_HTTP_PORT=80