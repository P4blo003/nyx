# ==========================================================================================
# Author: Pablo González García.
# Created: 31/12/2025
# Last edited: 15/01/2026
#
# Docker Compose file for deploying multiple Triton Inference Server instances.
#
# This setup separates inference workloads by nature:
# - Triton + vLLM for Large Language Models (LLMs).
# - Triton + ONNX Runtime for classical ML / DL models.
#
# The separation allows independent optimization, scaling and deployment.
# ==========================================================================================


# ==============================
# NETWORKS
# ==============================

networks:
  common-net:
    driver: bridge
  # Internal bridge network used for communication between Triton containers
  # and any auxiliary services (API gateways, orchestrators, etc).
  # This network is not intended to be exposed externally.
  triton-net:
    driver: bridge
  metrics-net:
    driver: bridge


# ==============================
# SERVICES
# ==============================

# Triton services.
services:

  # Prometheus is responsible for scraping, staring and querying metrics
  # exposed by Triton Inference Server instances.
  prometheus:

    # Official Prometheus image.
    # In production, pin a specific version instead of using 'latest'.
    # to ensure reproducible and stable deployments.
    image: prom/prometheus:v2.50.0
    # Explicit container name for easier management and monitoring.
    container_name: prometheus
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Attach the container to the internal networks.
    networks:
      - metrics-net

    # Mount Prometheus configuration file.
    # This file defines scrape targets, intervals and alerting rules.
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
  
  # Grafana is used exclusively for visualizing and alerting on top of
  # Prometheus metrics. It does not collect metrics by itself.
  grafana:

    # Official Grafana image.
    # In production, pin a specific version instead of using 'latest'.
    image: grafana/grafana:10.1.2
    # Explicit container name for easier management and monitoring.
    container_name: grafana
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Environment variables specific to Grafana.
    # See https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/ for details.
    environment:
      # Initial admin password for Grafana.
      # IMPORTANT:
      # - Do not hardcode credentials in production.
      # - Use Docker secrets, environment injection or external auth.
      - GF_SECURITY_ADMIN_PASSWORD=admin
      # Disable anonymous access to avoid exposing dashboards unintentionally.
      - GF_AUTH_ANONYMOUS_ENABLED=${GF_AUTH_ANONYMOUS_ENABLED}
      # Set Grafana to production mode.
      - GF_SERVER_ENABLE_GZIP=${GF_SERVER_ENABLE_GZIP}

    # Attach the container to the internal networks.
    networks:
      - metrics-net

    # Expose Grafana web UI.
    ports:
      - "3000:3000"

    # Mount Grafana volumes.
    volumes:
      - ./infra/grafana/provisioning/users/users.yaml:/etc/grafana/provisioning/users/users.yaml:ro
      # Optional: Mount dashboards, data persistence.
      - ./infra/grafana/data:/var/lib/grafana

  # Triton server dedicated to LLM inference using vLLM.
  # This service is expected to run on a GPU with sufficient memory and compute
  # capability to host Large Language Models efficiently.
  triton-vllm:

    # Triton Inference Server image including vLLM and Python backend support.
    # The version is controller via the TRITON_VERSION variable in the .env file.
    image: nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-vllm-python-py3
    # Explicit container name for easier management and monitoring.
    container_name: triton-vllm
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '8gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864
    
    # Environment variables specific to the vLLM workload.
    # See https://docs.vllm.ai/en/latest/configuration/env_vars/ for details.
    environment:
      # GPU device(s) visible to this container.
      # Typically a single, dedicated GPU is assigned to the LLM service.
      - CUDA_VISIBLE_DEVICES=${VLLM_CUDA_VISIBLE_DEVICES}
      # Attention backend used by vLLM.
      # XFORMERS is broadly compatible and stable across GPU generations.
      - VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND}

    # Attach the container to the internal networks.
    networks:
      - triton-net
      - metrics-net

    # Mount the model repository for vLLM-backend models.
    volumes:
      - ${MODEL_REPO}/vllm:/models:ro

    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true

    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  # Triton inference server.
  triton-onnx:
    # Triton Inference Server image with Python backend support.
    # ONNX Runtime and TensorRT backends are included in the standard image.
    image: nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-py3
    # Explicit container name for easier management and monitoring.
    container_name: triton-onnx
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '4gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864

    # Environment variables specific to the ONNX workload.
    environment:
      # GPU device(s) visible to this container.
      # When sharing a GPU with vLLM, ONNX models should be lightweight
      # and carefully tuned to avoid attention.
      - CUDA_VISIBLE_DEVICES=${ONNX_CUDA_VISIBLE_DEVICES}

    # Attach the container to the internal networks.
    networks:
      - triton-net
      - metrics-net

    # Mount the model repository for ONNX-backend models.
    volumes:
      - ${MODEL_REPO}/onnx:/models:ro

    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true

    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  nyx-ai:

    # Build image for this service using Dockerfile.
    build:
      context: ./services/ai
      dockerfile: Dockerfile
    # AI service to use.
    image: nyx/ai:latest
    # Explicit container name for easier management and monitoring.
    container_name: nyx-ai
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped
    
    # Environment variables specific to the ai service workload.
    environment:
      - VLLM_HOST=triton-vllm
      - VLLM_HTTP_PORT=8000
      - VLLM_GRPC_PORT=8001
      - ONNX_HOST=triton-onnx
      - ONNX_HTTP_PORT=8000
      - ONNX_GRPC_PORT=8001

    # Attach the container to the internal networks.
    networks:
      - common-net
      - triton-net
      - metrics-net

    # Expose the FastAPI port to the host for external access.
    ports:
      - "${AI_HTTP_PORT}:80"

    # Mount the model repository for develop scripts.
    volumes:
      - ./services/ai/src:/app/src
      - ${MODEL_REPO}:/app/models
    
    # Start ai service.
    command: >
      python src/main.py

    # Ensure Triton containers start before this service
    # Note: This does not guarantee Triton is fully ready
    depends_on:
      - triton-onnx
      - triton-vllm

  nyx-rag:
    
    # Build image for this service using Dockerfile.
    build:
      context: ./services/rag
      dockerfile: Dockerfile
    # Rag Service to use.
    image: nyx/rag:latest
    # Explicit container name for easier management and monitoring.
    container_name: nyx-rag
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Environment variables specific to the ai service workload.
    environment:
      - AI_SERVICE_HOST=nyx-ai
      - AI_SERVICE_PORT=${AI_HTTP_PORT}

    # Attach the container to the internal networks.
    networks:
      - common-net
      - metrics-net

    # Expose the FastAPI port to the host for external access.
    ports:
      - "${RAG_HTTP_PORT}:80"
    
    # Mount the model repository for develop scripts.
    volumes:
      - ./services/rag/src:/app/src

    # Start rag service.
    command: >
      python src/main.py