# ==========================================================================================
# Author: Pablo González García.
# Created: 31/12/2025
# Last edited: 02/02/2026
#
# Docker Compose file for deploying multiple Triton Inference Server instances.
#
# This setup separates inference workloads by nature:
# - Triton + vLLM for Large Language Models (LLMs).
# - Triton + ONNX Runtime for classical ML / DL models.
#
# The separation allows independent optimization, scaling and deployment.
# ==========================================================================================


# ==============================
# NETWORKS
# ==============================

networks:
  # Shared network for inter-service communication between core application services.
  common-net:
    driver: bridge
  # Internal bridge network used for communication between Triton containers
  # and any auxiliary services (API gateways, orchestrators, etc).
  # This network is not intended to be exposed externally.
  triton-net:
    driver: bridge
  # Network dedicated to metrics, monitoring and observability components.
  metrics-net:
    driver: bridge
  
  cache-net:
    driver: bridge


# ==============================
# SERVICES
# ==============================

services:

  dcgm-exporter:
    build:
      context: ./third-party/dcgm-exporter
      dockerfile: Dockerfile
    
    container_name: dcgm-exporter

    environment:
      - DCGM_EXPORTER_INTERVAL=1000

    networks:
      - metrics-net

    ports:
      - "9400:9400"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  prometheus:
    build:
      context: ./third-party/prometheus
      dockerfile: Dockerfile
    container_name: prometheus

    environment:
      - name=value

    restart: unless-stopped

    networks:
      - metrics-net

    volumes:
      - ./third-party/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./third-party/prometheus/data:/prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=6h'
      - '--web.enable-lifecycle'
  
  grafana:
    build:
      context: ./third-party/grafana
      dockerfile: Dockerfile
    container_name: grafana

    restart: unless-stopped

    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=1s

    networks:
      - metrics-net

    ports:
      - "3000:3000"

    volumes:
      - ./third-party/grafana/provisioning:/etc/grafana/provisioning
      - ./third-party/grafana/data:/var/lib/grafana

  # Triton Inference Server for embedding and classification models.
  triton-onnx:

    # Build image for this service using Dockerfile.
    build:
      context: ./third-party/triton/onnx
      dockerfile: Dockerfile
    # Explicit container name for easier management and monitoring.
    container_name: triton-onnx
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '4gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864

    # Environment variables specific to the ONNX workload.
    environment:
      # GPU device(s) visible to this container.
      # When sharing a GPU with vLLM, ONNX models should be lightweight
      # and carefully tuned to avoid attention.
      - CUDA_VISIBLE_DEVICES=0
    
    # Attach the container to the shared internal networks.
    networks:
      - triton-net
      - metrics-net

    # Persistent storage and configuration.
    volumes:
      # Mount the model repository for ONNX-backend models.
      - ./model_repository/onnx:/models:ro
    
    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true
    
    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  ai-service:
    build:
      context: ./services/ai-service
      dockerfile: Dockerfile
    # Explicit container name for easier management and monitoring.
    container_name: ai-service
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    networks:
      - common-net
      - triton-net
      - metrics-net

    ports:
      # Expose the AI-Service API.
      - "${AI_SERVICE_GRPC_PORT}:8002"

    volumes:
      # Mount development directories.
      - ./services/ai-service/logs:/app/logs
      - ./services/ai-service/src:/app/src
      # Mount the application configuration directory (read-only).
      - ./services/ai-service/config:/app/config:ro

    command: >
      python src/main.py