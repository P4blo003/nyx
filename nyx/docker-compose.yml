# ==========================================================================================
# Author: Pablo González García.
# Created: 31/12/2025
# Last edited: 29/01/2026
#
# Docker Compose file for deploying multiple Triton Inference Server instances.
#
# This setup separates inference workloads by nature:
# - Triton + vLLM for Large Language Models (LLMs).
# - Triton + ONNX Runtime for classical ML / DL models.
#
# The separation allows independent optimization, scaling and deployment.
# ==========================================================================================


# ==============================
# NETWORKS
# ==============================

networks:
  # Shared network for inter-service communication between core application services.
  common-net:
    driver: bridge
  # Internal bridge network used for communication between Triton containers
  # and any auxiliary services (API gateways, orchestrators, etc).
  # This network is not intended to be exposed externally.
  triton-net:
    driver: bridge
  # Network dedicated to metrics, monitoring and observability components.
  metrics-net:
    driver: bridge


# ==============================
# SERVICES
# ==============================

services:

  # Qdrant vector database service.
  # Used for storing and querying embeddings for semantic search and RAG workflow.
  qdrant:
    # Default configuration.
    image: qdrant/qdrant:v1.16.3
    # Explicit container name for easier management and monitoring.
    container_name: qdrant
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Environment variables for service configuration.
    environment:
      # API key used to authenticate client requests.
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
      # Enable or disable TLS support (true / false).
      - QDRANT__SERVICE__ENABLE_TLS=${QDRANT_ENABLE_TLS}

    # Attach the service to the shared internal networks.
    networks:
      - common-net

    # Expose HTTP and gRPC ports to the host.
    ports:
      # 6333: REST / HTTP API.
      - "6333:6333"
      # 6334: gRPC API.
      - "6334:6334"

    # Persistent storage and configuration.
    volumes:
      # Persistent data directory for collections, indexes and WAL.
      - ./third-party/qdrant/data:/qdrant/storage
      #    Production configuration file (read-only).
      - ./third-party/qdrant/config/production.yaml:/qdrant/config/production.yaml:ro

    # Health check to ensure the service is alive and responsive.
    # Uses the built-in Qdrant healthcheck endpoint.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s

    # Resource limits to prevent the service from exhausting host resources.
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 4g
  
  # Triton Inference Server for embedding and classification models.
  triton-onnx:

    # Build image for this service using Dockerfile.
    build:
      context: ./third-party/triton/onnx
      dockerfile: Dockerfile
    # Explicit container name for easier management and monitoring.
    container_name: triton-onnx
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '4gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864

    # Environment variables specific to the ONNX workload.
    environment:
      # GPU device(s) visible to this container.
      # When sharing a GPU with vLLM, ONNX models should be lightweight
      # and carefully tuned to avoid attention.
      - CUDA_VISIBLE_DEVICES=0
    
    # Attach the container to the shared internal networks.
    networks:
      - triton-net
      - metrics-net

    # Persistent storage and configuration.
    volumes:
      # Mount the model repository for ONNX-backend models.
      - ./model_repository/onnx:/models:ro
    
    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true
    
    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  
  ai-service:
    
    # Build image for this service using Dockerfile.
    build:
      context: ./services/ai
      dockerfile: Dockerfile
    # Explicit container name for easier management and monitoring.
    container_name: ai-service
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Environment variables specific to the AI Service workload.
    environment:
      - VLLM_HOST=triton-vllm
      - VLLM_HTTP_PORT=8000
      - VLLM_GRPC_PORT=8001
      - ONNX_HOST=triton-onnx
      - ONNX_HTTP_PORT=8000
      - ONNX_GRPC_PORT=8001

    # Attach the container to the shared internal networks.
    networks:
      - triton-net
      - metrics-net
      - common-net

    # Expose the FastAPI port to the host for external access.
    ports:
      - "1080:80"

     # Persistent storage and configuration.
    volumes:
      - ./services/ai/src:/app
      - ./services/ai/config:/app/config
    
    # Start ai service.
    command: >
      python -m uvicorn main:app --host 0.0.0.0 --port 80

    # Ensure Triton containers start before this service
    # Note: This does not guarantee Triton is fully ready
    depends_on:
      - triton-onnx