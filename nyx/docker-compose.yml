# ==========================================================================================
# Author: Pablo González García.
# Created: 31/12/2025
# Last edited: 29/01/2026
#
# Docker Compose file for deploying multiple Triton Inference Server instances.
#
# This setup separates inference workloads by nature:
# - Triton + vLLM for Large Language Models (LLMs).
# - Triton + ONNX Runtime for classical ML / DL models.
#
# The separation allows independent optimization, scaling and deployment.
# ==========================================================================================


# ==============================
# NETWORKS
# ==============================

networks:
  common-net:
    driver: bridge
  # Internal bridge network used for communication between Triton containers
  # and any auxiliary services (API gateways, orchestrators, etc).
  # This network is not intended to be exposed externally.
  triton-net:
    driver: bridge
  metrics-net:
    driver: bridge


# ==============================
# SERVICES
# ==============================

# Triton services.
services:

  # Triton server dedicated to LLM inference using vLLM.
  # This service is expected to run on a GPU with sufficient memory and compute
  # capability to host Large Language Models efficiently.
  triton-vllm:

    # Triton Inference Server image including vLLM and Python backend support.
    # The version is controller via the TRITON_VERSION variable in the .env file.
    image: nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-vllm-python-py3
    # Explicit container name for easier management and monitoring.
    container_name: triton-vllm
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '8gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864
    
    # Environment variables specific to the vLLM workload.
    # See https://docs.vllm.ai/en/latest/configuration/env_vars/ for details.
    environment:
      # GPU device(s) visible to this container.
      # Typically a single, dedicated GPU is assigned to the LLM service.
      - CUDA_VISIBLE_DEVICES=${VLLM_CUDA_VISIBLE_DEVICES}
      # Attention backend used by vLLM.
      # XFORMERS is broadly compatible and stable across GPU generations.
      - VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND}

    # Attach the container to the internal networks.
    networks:
      - triton-net
      - metrics-net

    # Mount the model repository for vLLM-backend models.
    volumes:
      - ${MODEL_REPO}/vllm:/models:ro

    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true

    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  # Triton inference server.
  triton-onnx:

    # Build image for this service using Dockerfile.
    build:
      context: ./infra/triton/onnx
      dockerfile: Dockerfile
    # Explicit container name for easier management and monitoring.
    container_name: triton-onnx
    # Enable NVIDIA Container runtime for GPU access.
    runtime: nvidia
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Increase shared memory size inside the container.
    # Triton, CUDA, TensorRt, ONNX Runtime and vLLM rely heavily on /dev/shm.
    # For inter-process communication, tensor exchange and dynamic batching.
    # The default Docker value (64 MB) is insufficient for ML inference workloads.
    shm_size: '4gb'
    ulimits:
      # Allows the container to lock unlimited amounts of memory.
      # This is required by CUDA and TensorRT to pin memory and prevent it
      # from being swapped out by the OS.
      memlock: -1
      # Increase the maximum stack size per thread to 64 MB.
      # Deep C++ call stacks used by Triton, TensorRT and ONNX Runtime
      # can overflow the default Linux stack limit (usually 8MB).
      stack: 67108864

    # Environment variables specific to the ONNX workload.
    environment:
      # GPU device(s) visible to this container.
      # When sharing a GPU with vLLM, ONNX models should be lightweight
      # and carefully tuned to avoid attention.
      - CUDA_VISIBLE_DEVICES=${ONNX_CUDA_VISIBLE_DEVICES}

    # Attach the container to the internal networks.
    networks:
      - triton-net
      - metrics-net

    # Mount the model repository for ONNX-backend models.
    volumes:
      - ${MODEL_REPO}/onnx:/models:ro

    # Start Triton with explicit runtime options.
    # Metrics are enabled for observability and capacity planning.
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --log-info=true
      --allow-metrics=true
      --allow-gpu-metrics=true

    # Health check to ensure the Triton server is fully ready
    # before traffic is sent to this service.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  nyx-ai:

    # Build image for this service using Dockerfile.
    build:
      context: ./services/ai
      dockerfile: Dockerfile
    # AI service to use.
    image: nyx/ai:latest
    # Explicit container name for easier management and monitoring.
    container_name: nyx-ai
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped
    
    # Environment variables specific to the ai service workload.
    environment:
      - VLLM_HOST=triton-vllm
      - VLLM_HTTP_PORT=8000
      - VLLM_GRPC_PORT=8001
      - ONNX_HOST=triton-onnx
      - ONNX_HTTP_PORT=8000
      - ONNX_GRPC_PORT=8001

    # Attach the container to the internal networks.
    networks:
      - common-net
      - triton-net
      - metrics-net

    # Expose the FastAPI port to the host for external access.
    ports:
      - "${AI_HTTP_PORT}:80"

    # Mount the model repository for develop scripts.
    volumes:
      - ./services/ai/src:/app
      - ./services/ai/config:/app/config
    
    # Start ai service.
    command: >
      python -m uvicorn main:app --host 0.0.0.0 --port 80

    # Ensure Triton containers start before this service
    # Note: This does not guarantee Triton is fully ready
    depends_on:
      - triton-onnx
      - triton-vllm

  qdrant:
    # Default configuration.
    image: qdrant/qdrant:${QDRANT_VERSION}
    container_name: qdrant
    restart: unless-stopped

    # Networks.
    networks:
      - common-net

    # Container ports.
    ports:
      - "${QDRANT_HTTP_PORT}:6333"
      - "${QDRANT_GRPC_PORT}:6334"

    # Volumes.
    volumes:
      - ./infra/qdrant/data:/qdrant/storage
      - ./infra/qdrant/config/qdrant.yaml:/qdrant/config/production.yaml:ro

    environment:
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
      - QDRANT__SERVICE__ENABLE_TLS=${QDRANT_TLS_ENABLED}

    # Container healthcheck.
    healthcheck:
      test: ["CMD", "/bin/sh", "/healthcheck.sh"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s

    # Deploys.
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 4g

  
  nyx-rag:

    # Build image for this service using Dockerfile.
    build:
      context: ./services/rag
      dockerfile: Dockerfile
    # AI service to use.
    image: nyx/rag:latest
    # Explicit container name for easier management and monitoring.
    container_name: nyx-rag
    # Automatically restart the container unless it is explicitly stopped.
    restart: unless-stopped

    # Attach the container to the internal networks.
    networks:
      - common-net
      - metrics-net

    # Expose the FastAPI port to the host for external access.
    ports:
      - "${RAG_HTTP_PORT}:80"

    # Mount the model repository for develop scripts.
    volumes:
      - ./services/rag/src:/app
    
    # Start ai service.
    command: >
      python -m uvicorn main:app --host 0.0.0.0 --port 80

    # Ensure Triton containers start before this service
    # Note: This does not guarantee Triton is fully ready
    depends_on:
      - qdrant